name: Shoob Scraper

on:
  workflow_dispatch:
    inputs:
      start:
        description: "Start page"
        required: true
        default: "1"
      end:
        description: "End page"
        required: true
        default: "100"

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 360

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          playwright install chromium

      - name: Run scraper
        run: |
          python main.py \
            --start ${{ github.event.inputs.start }} \
            --end ${{ github.event.inputs.end }} \
            --resume

      # ðŸ”’ GUARANTEED SAVE (runs even if scraper fails)
      - name: Upload output as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: shoob-data-${{ github.run_number }}
          path: output/
          retention-days: 90

      # ðŸ“Œ Commit only if scraper step succeeded
      - name: Commit output
        if: success()
        run: |
          git config user.name "github-actions"
          git config user.email "actions@github.com"
          git add output/
          git commit -m "Scraped pages ${{ github.event.inputs.start }}-${{ github.event.inputs.end }}" || echo "No changes"
          git push
